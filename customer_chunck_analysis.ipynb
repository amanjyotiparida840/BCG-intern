{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here all of those package import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "%matplotlib inline\n",
    "data=pd.read_csv('ml_case_training_data.csv')\n",
    "data1=pd.read_csv('ml_case_training_output.csv')\n",
    "df=pd.merge(data,data1,on='id')\n",
    "pd.set_option('display.max_columns', None)\n",
    "df.drop('id',axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here,I checked how much percentage(%) of nan values present in each feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nan_feature=[feature for feature in df.columns if df[feature].isnull().sum()>1]\n",
    "# # print(len(nan_feature))\n",
    "\n",
    "# for feature in nan_feature:\n",
    "#     print(feature,\" has \",np.round(df[feature].isnull().mean(),4),\"% nan value\")\n",
    "\n",
    "# for feature in df.columns:\n",
    "#     print(f'{feature}: {df[feature].isnull().sum()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here, I droped those feature who have contained more than 70% of nan value and handle catagorical value and apply one hot encoding because its nominal. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def drop_column(data,threshold):\n",
    "#     nan_feature=[feature for feature in data.columns if data[feature].isnull().sum()>1]\n",
    "#     for feature in nan_feature:\n",
    "#         if(np.round(df[feature].isnull().mean())>threshold):\n",
    "#             df.drop(columns=feature,inplace=True)\n",
    "#     return data\n",
    "# drop_column(df,0.7)\n",
    "\n",
    "# df['channel_sales'].isnull().sum()\n",
    "# df['channel_sales'].mode()[0]\n",
    "\n",
    "# df['channel_sales'].fillna(df['channel_sales'].mode()[0],inplace=True)\n",
    "# df['origin_up'].fillna(df['origin_up'].mode()[0],inplace=True)\n",
    "\n",
    "\n",
    "# for feature in df.columns:\n",
    "#     if(df[feature].isnull().sum()>1 and 'forecast' in feature):\n",
    "#         df[feature].fillna(df[feature].mean(),inplace=True)\n",
    "\n",
    "# df.dropna()\n",
    "\n",
    "# for feature in df.columns:\n",
    "#     print(feature,df[feature].isnull().sum())\n",
    "\n",
    "# catagorical_feature=[feature for feature in df.columns if df[feature].dtype=='O' and  len(df[feature].unique())<25]\n",
    "# catagorical_feature\n",
    "\n",
    "# df1=pd.get_dummies(df.channel_sales,drop_first=True)\n",
    "# df2=pd.get_dummies(df.has_gas,drop_first=True)\n",
    "# df3=pd.get_dummies(df.origin_up,drop_first=True)\n",
    "# merge_data=pd.concat([df,df1,df2,df3],axis=1)\n",
    "# merge_data.drop(['channel_sales','has_gas','origin_up'],axis=1,inplace=True)\n",
    "\n",
    "# merge_data.dropna(inplace=True)\n",
    "# merge_data.isnull().sum()\n",
    "# merge_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here i convert the data type of object into date time because it caontains only year month and day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [],
   "source": [
    "# date_time=[feature for feature in df.columns if 'date' in feature and df[feature].dtype=='O']\n",
    "# for i in range(len(date_time)):\n",
    "#     merge_data[date_time[i]]=pd.to_datetime(merge_data[date_time[i]])\n",
    "# date_time=[feature for feature in df.columns if 'date' in feature]\n",
    "# for i in range(len(date_time)):\n",
    "#     merge_data[date_time[i]+\"_year\"]=merge_data[date_time[i]].dt.year\n",
    "#     merge_data[date_time[i]+\"_month\"]=merge_data[date_time[i]].dt.month\n",
    "#     merge_data[date_time[i]+\"_day\"]=merge_data[date_time[i]].dt.day  \n",
    "# merge_data.drop(['date_activ','date_end','date_modif_prod','date_renewal'],axis=1,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.figure(figsize=(12,6))\n",
    "# ax=sns.countplot(data=df,x='channel_sales',hue='churn',palette='mako')\n",
    "# bx=ax.set_xticklabels(ax.get_xticklabels(), rotation=20, ha=\"right\",fontsize=20)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# ax=sns.countplot(data=df,x='origin_up',hue='churn',palette='mako')\n",
    "# bx=ax.set_xticklabels(ax.get_xticklabels(), rotation=20, ha=\"right\",fontsize=20)\n",
    "\n",
    "\n",
    "# plt.figure(figsize=(12,6))\n",
    "# ax=sns.countplot(data=df,x='has_gas',hue='churn',palette='mako')\n",
    "# bx=ax.set_xticklabels(ax.get_xticklabels(), rotation=20, ha=\"right\",fontsize=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Here is the mean encoding stratergy which is more usefull ,when feature size is more than 50 or 100,basicaly when we does not want to increase our dimeensioanlity of our data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# df.channel_sales.value_counts().to_dict()\n",
    "# mean_encoded_=df.groupby(['channel_sales'])['churn'].mean()\n",
    "# df['channel_sales']=df['channel_sales'].map(mean_encoded_)\n",
    "# df['channel_sales'].dtypes\n",
    "# df['channel_sales'].fillna(df['channel_sales'].value_counts().idxmax(),inplace=True)\n",
    "# df['channel_sales'].isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intersection(lst1, lst2): \n",
    "    return list(set(lst1) & set(lst2))\n",
    "# intersection(continious_feature,time_feature)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
